{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columbia University\n",
    "### ECBM E4040 Neural Networks and Deep Learning. Fall 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1, Task 4: Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "Cross entropy is a metric that measures the \"distance\" between two distributions, why can it be used in calculating the loss of softmax classifier? \n",
    "\n",
    "   Your answer: \n",
    "   The reason is that we are considering two discrete probability distribution y and y_hat.The output from softmax function y_hat is a number between 0 and 1 which demonstrates the probability of how likely the input belongs to 1 or 0. The closer the output is to 1, the more likely the input belongs to the class 1. And the true label y is either 1 or 0.Therefore, the mutiplication between y and log of the probability y-hat which is exactly the form of cross-entropy can show the difference between the the distribution of the predictions and the true label. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 \n",
    "Please first describe the difference between multi-class and binary logistic regression; then describe another possible way to derive a multi-class logistic regression classifier from a binary one; finally, illustrate how they work in a deep learning classification model.\n",
    "\n",
    "   Your answer: \n",
    "   The label y for the binary logistic regression is either 1 or 1 while the label y for the multiple regression is an integer in {1,...,C} where C is the number of classes. \n",
    "   \n",
    "   For a sample x,the class scores are z=Wx+b.Then we use softmax function to get the predicted probability distribution from the class scores: y_hat=softmax(z).Next we can compute the loss for the training sample x using the cross entropy loss function: loss=-sum(y_i* log(y_i_hat)). Our goal is to make the loss as minimum as possible. Therefore, we learn the parameters W and b by performing gradient descent on the loss function with respect to these parameters.As in the binary logistic regression case, the loss function is convex, so gradient descent will converge to a global minimum with a small enough step size.With L2-regularization on W, the loss becomes strictly convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "Why is the ReLU activation function used the most often in neural networks for computer vision?\n",
    "\n",
    "   Your answer: \n",
    "   The ReLU function is f(x)=max(0,x).One way ReLUs improve neural networks is by speeding up training. The gradient computation is very simple, which is either 0 or 1 depending on the sign of x. Also,the computational step of a ReLU is easy: any negative elements are set to 0. Futhermore, modified ReLU units, leaky ReLU, for example, can ameliorate the problem of dead neurons which results from the smaller gradients of other activation functions.\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "**Cross validation** is a technique used to prove the generalization ability of a model and can help you find a robust set of hyperparameters. Please describe the implementation details of **k-fold cross validation**.\n",
    "\n",
    "   Your answer:\n",
    "   Faced with dataset with n samples, we randomly choose one part k as validation set where there are n/k samples, and the other part k-1 for training.After choosing the validation set, we need to calculate the MSE of the selected part k. We need to repeat the above process for k times and average the MSE error to get the final error result.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Describe your best model in the implementation of the two-layer neural network. Describe your starting point, how you tuned  hyperparameters, which stategies you used to improve the network, show the results of intermediate and the final steps.\n",
    "\n",
    "   Your answer: \n",
    "   My model has one 3072 inputs and 10 outputs indicating 10 different classes. There is one hidden layer in it with 200 inputs.In my model, the first adjustment I do is increasing the number of training or epoch times from 10 to 20. The more the model is trained, the better the accuracy would be.After trying many times I finally achieved the accuracy of 51.35% by increasing the learning rate a little bit to 10e-4 and reduced the regularization a little bit from 1e-4 to 1e-5. Learning rate is a hyper-parameter that controls how much we are adjusting the weights of our network with respect the loss gradient. The lower the value, the slower we travel along the downward slope. \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "(Optional, this question is included in the 10 points bonus) In tSNE, describe the motivation of tuning the parameter and discuss the difference in results you see.\n",
    "    \n",
    "   Your answer: **[fill in here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
